---
title: "Topics in Optimization: Mathematical Foundations of Machine Learning"
date: 2025-09-01T00:00:00-0700
tags:
- 2025-2026
categories:
- upcoming
featured: false
draft: false
registration_open: true
publish_date: 2025-06-03T00:00:00-0800
course_title: "Discrete Optimization"
instructors:
- name: Andrew Warren
  institution: University of British Columbia
  email: awarren@math.ubc.ca
course_start: '2025-09-03'
course_end: '2025-12-03'
course_number: MATH 604
section_number: ''
section_code: ''
prerequisites:
  - Mathematical maturity at the second year master's level or higher
  - Measure theory

abstract: >
  This course is a bridge into the machine learning literature for graduate
  students in mathematics. Compared to existing course offerings in our
  neighbouring departments (mainly https://www.cs.ubc.ca/~dsuth/532D/23w1
  (https://www.cs.ubc.ca/~dsuth/532D/23w1)) we will assume that you know somewhat
  more analysis, but prior coding experience will not be required.

  Briefly, the learning objectives are:
    - understand the different "learning paradigms" considered in ML (supervised learning, unsupervised learning, reinforcement learning, etc.) and their relation with existing statistical theory
    - be comfortable with mathematical tools (eg. kernel methods) which appear commonly in the ML literature but are not well known among pure mathematicians
    - see some natural connections between ML theory and: optimization/calculus of variations, measure theory, PDE, etc
    - gain fluency reading ML papers (which can be less trustworthy than pure math papers)
    - start to think about how to bring your area of mathematical expertise to bear on ML problems.
#syllabus_pdf: syllabus.pdf
syllabus_txt: >
  ### Outline:
    * Unit 0: (~1 week) What is machine learning?
    * Unit 1: (~4 weeks) Supervised learning: The statistical learning theory
      framework. Inference in high dimension. Falsibiability of models and measures
      of model complexity. Regression and classification. Kernel methods. Learning
      with neural networks. Double-descent and failure of Ockham's razor.
    * Unit 2: (~3 weeks) Unsupervised learning: Clustering and dimensionality
      reduction. Manifold hypothesis. Geometric graph methods. Inferring probability distributions: density estimation, sampling, generative models.
    * Unit 3: (~4 weeks) Reinforcement learning: Exploration-exploitation tradeoff. Sequential decision problems. Markov decision processes and connections with control theory. Efficient exploration for bandit problems and small-scale games. Complexity notions and learnability for large scale games.
  ### Main references: for textbook references we will use a couple chapters from each.
    * Unit 0: Vapnik, "The nature of statistical learning theory".
    * Unit 1: Wainwright, "High-dimensional statistics". Bach, "Learning theory from first principles".
    * Unit 2: There is no good textbook for unsupervised learning that I am aware of. I have course notes. We will also look at some classic research papers, for example for geometric graph methods we will read "Laplacian eigenmaps for dimensionality reduction and data representation" by Belkin and Niyogi.
    * Unit 3: Foster and Rakhlin, RL theory notes: [https://arxiv.org/abs/2312.16730](https://arxiv.org/abs/2312.16730)
---


### Class Schedule
  * TBA

### Remote Access
Remote access to this course will be via zoom. The delivery mechanism will be
either blackboard or via tablet depending on available rooms. A PDF textbook
and/or research article readings will be distributed in advance of each class.

### Availability
This course may be open to students from universities outside of the PIMS network.

